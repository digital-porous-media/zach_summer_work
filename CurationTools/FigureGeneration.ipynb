{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42483b0-7c34-45a5-b00a-209e666dfb08",
   "metadata": {},
   "source": [
    "### These are all the functions used to create graphs. Specifically those found in the abstract written for the Data Curation Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a039fd-0c9a-4681-971d-e477d449dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries for this notebook\n",
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971449f-aa8a-464c-a177-a065c5d11f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and load the json file containing all the evaluations as provided by the LlamaReviews script.\n",
    "filename = './ReviewsExample.json'\n",
    "with open(filename,'r') as file:\n",
    "    scoring = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26549c6-368f-47b3-ba48-de1a7984b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a list containing one total score for each of the data descriptions\n",
    "def get_total_scores():\n",
    "    individual_scores = []\n",
    "    for score in scoring:\n",
    "        breakdown = eval(scoring.get(score).get('breakdown'))\n",
    "        full_score = 0\n",
    "        for guideline in breakdown:\n",
    "            full_score += guideline[1]\n",
    "        individual_scores.append(full_score)\n",
    "    return individual_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f91281-949b-4621-a8ee-f64f7f8ce763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the average of the scores\n",
    "def get_average():\n",
    "    scores = get_total_scores()\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "get_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cafb2d-b627-4711-a41e-c9d1b81d0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides the correctly formatted distribution of scores for the guideline-by-guideline breakdown graph\n",
    "def get_guideline_distribution():\n",
    "    guideline_breakdown = [[0,0,0] for i in range(10)]\n",
    "    for score in scoring:\n",
    "        for guide in eval(scoring.get(score).get('breakdown')):\n",
    "            if guide[1]==0:\n",
    "                guideline_breakdown[guide[0]-1][0]+=1\n",
    "            if guide[1]==0.5:\n",
    "                guideline_breakdown[guide[0]-1][1]+=1\n",
    "            if guide[1]==1:\n",
    "                guideline_breakdown[guide[0]-1][2]+=1\n",
    "    \n",
    "    zeros = [score[0] for score in guideline_breakdown]\n",
    "    halves = [score[1] for score in guideline_breakdown]\n",
    "    fulls = [score[2] for score in guideline_breakdown]\n",
    "    # If you want to add values for guideline 11 (word count) you can do that here\n",
    "    # zeros.append(164)\n",
    "    # halves.append(0)\n",
    "    # fulls.append(12)\n",
    "    return (zeros,halves,fulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36091f-9139-4742-8a56-59516506a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will generate you a histogram of all of the data description scores\n",
    "plt.hist(get_total_scores(), bins=[i/2 for i in range(0,21,1)], color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Data Description Score\")\n",
    "plt.ylabel(\"Dataset Description Count\")\n",
    "plt.title(\"Graph of Data Description Scores for DPMP\")\n",
    "plt.xlim(0,10)\n",
    "plt.xticks(range(11))\n",
    "plt.show()\n",
    "# plt.savefig('score_distribution.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6caa6-aaea-473d-a5f3-9406c0ccf2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the guideline-by-guideline breakdown figure\n",
    "categories = [str(i+1) for i in range(10)]\n",
    "\n",
    "distribution = get_guideline_distribution()\n",
    "\n",
    "bar_width = 0.25\n",
    "index = np.arange(10)\n",
    "\n",
    "plt.bar(index - bar_width, distribution[0], bar_width, label='No Point',color='#e57373')\n",
    "plt.bar(index, distribution[1], bar_width, label='Half Point',color='#fdd835')\n",
    "plt.bar(index + bar_width, distribution[2], bar_width, label='Full Point',color='#81c784')\n",
    "\n",
    "plt.xlabel('Guideline Number')\n",
    "plt.ylabel('Dataset Description Count')\n",
    "plt.title('Score Breakdown by Guideline')\n",
    "plt.xticks(index, categories)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('guideline_breakdown_final.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97853ad8-e06c-449c-b0a7-b939c7ddbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the presentation of a data description and its evaluation as seen in the abstract\n",
    "def make_evaluation(DRP):\n",
    "    evaluation = scoring.get(DRP).get('eval')\n",
    "    desc = scoring.get(DRP).get('orig')\n",
    "    breakdown = eval(scoring.get(DRP).get('breakdown'))\n",
    "    score = 0\n",
    "    for guideline in breakdown:\n",
    "        score+=guideline[1]\n",
    "    print(f\"#### {DRP}\\n\\nOriginal Description: {desc}\\n\\nLLM Evaluation: {evaluation}\\n\\nFinal Score: {score}\")\n",
    "\n",
    "# make_evaluation(\"DRP-333.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74ed04-448f-4b95-b4c3-5ef7605caeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what percentage of description scored the provided threshold or higher (so it is inclusive)\n",
    "def check_percentage_above(threshold):\n",
    "    good = 0\n",
    "    individual_scores = get_total_scores()\n",
    "    for score in individual_scores:\n",
    "        if score >= threshold:\n",
    "            good+=1\n",
    "    return good/len(individual_scores)\n",
    "\n",
    "# check_percentage_above(8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
